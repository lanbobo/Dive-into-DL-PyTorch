{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**已有数据初始化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据python的list初始化 (deep-copy: 内存拷贝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  4,  6,  8, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(range(2, 11, 2)) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据numpy的ndarray初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5] \n",
      "\n",
      " tensor([[ 1.5000,  2.5000,  3.5000,  4.5000,  5.5000],\n",
      "        [ 6.5000,  7.5000,  8.5000,  9.5000, 10.5000]], dtype=torch.float64) \n",
      "\n",
      " tensor([[11., 12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19., 20.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(1, 10, 10)\n",
    "# view不改变内存，只是重新设置了shape。\n",
    "y1 = torch.from_numpy(x).view(2 ,5)  # shallow-copy: 内存共享; from_numpy()的反操作为tensorVar.numpy()\n",
    "y2 = torch.tensor(x).view(2 ,5)  # deep-copy: 内存拷贝\n",
    "y1 += 0.5\n",
    "y2 += 10\n",
    "\n",
    "print(x, '\\n\\n', y1, '\\n\\n', y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用cpu/gpu的变量初始化gpu/cpu变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.] \n",
      "\n",
      "\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], dtype=torch.float64) \n",
      "\n",
      "\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], device='cuda:0',\n",
      "       dtype=torch.float64) \n",
      "\n",
      "\n",
      "tensor([ 2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.], device='cuda:0',\n",
      "       dtype=torch.float64) \n",
      "\n",
      "\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], dtype=torch.float64) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(1, 10, 10)  # np创建性函数大都默认float64类型\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "y = torch.tensor(x)\n",
    "z = y.to(device)  # 用cpu的变量初始化gpu变量  (deep-copy: 内存拷贝)\n",
    "\n",
    "print(id(x)==id(y), id(y)==id(z))\n",
    "print(x, '\\n\\n')\n",
    "print(y, '\\n\\n')\n",
    "print(z, '\\n\\n')\n",
    "print(y.to(device)+z, '\\n\\n')  # y+z : error--due to different device type\n",
    "print(z.to('cpu'), '\\n\\n') # 用gpu的变量初始化cpu变量  (deep-copy: 内存拷贝)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**函数创建式：Tensor/empty/zeros/ones/diag/linspace/arange/rand/randn/randint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 3.1964e-18, 3.0952e-41],\n",
      "        [3.1964e-18, 3.0952e-41, 3.1964e-18, 3.0952e-41]]) \n",
      "\n",
      " tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(2, 4)\n",
    "y = torch.Tensor([1, 2, 3])\n",
    "print(x, '\\n\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0354e-19,  4.5829e-41, -3.0354e-19],\n",
      "        [ 4.5829e-41,  4.4842e-44,  0.0000e+00],\n",
      "        [ 4.4842e-44,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -3.0354e-19,  4.5829e-41],\n",
      "        [ 3.8016e-39,  0.0000e+00,  4.6243e-44]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((2, 4)) == torch.zeros(2, 4)  # *size即可已接受一串整数，也能接受list或tuple\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4, 1, dtype=torch.float64)  # 可以指定dtype, 默认float32\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 3., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.diag(torch.tensor([1.0, 2.0, 3.0]), diagonal=-1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.,  6.,  8., 10.],\n",
      "        [12., 14., 16., 18., 20.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(2, 20 ,steps=10, dtype=torch.float32).view(2, 5)\n",
    "# linspace的dtype必须为float(32/64)类型，其他类型未实现\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.,  6.,  8., 10.],\n",
      "        [12., 14., 16., 18., 20.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(2, 20.001 ,step=2, dtype=torch.float32).view(-1, 5) \n",
    "# arange的dtype必须为float(32/64)类型，其他类型未实现\n",
    "# -1 代表该维度可由别的维度推断出来\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 6)  # 标准分布\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5636,  1.1431,  0.8590,  0.7056, -0.3406, -1.2720]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 6)  # 正态分布\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 2, 1, 4],\n",
      "        [2, 5, 4, 2],\n",
      "        [5, 3, 5, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1, 6, [3, 4])  # 随机整数\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算术操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6397, 1.9743, 1.8300],\n",
       "        [1.0444, 1.0246, 1.2588],\n",
       "        [1.9391, 1.4167, 1.7140]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x  = torch.ones(3, 3)\n",
    "y = torch.rand_like(x)\n",
    "z = x + y  # 加法形式 1\n",
    "torch.add(x, y)  # 加法形式 2\n",
    "y.add_(x)  # 加法形式 3, 等价于 y[:] = x + y, torch.add(x, y, out=y)： y和加法后的y指向相同地址的内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4757, -1.8821, -0.7765,  2.0242],\n",
      "        [-0.0865,  0.0981, -1.2150,  0.7312],\n",
      "        [-0.6298,  2.4070,  0.2786,  0.2468],\n",
      "        [ 1.1843, -0.7282,  1.1633, -0.0091]]) \n",
      "\n",
      " tensor([-0.4757, -1.8821, -0.7765,  2.0242]) \n",
      "\n",
      " tensor([-0.0865, -0.6298]) \n",
      "\n",
      " tensor([-0.4757,  0.0981,  0.2786, -0.0091])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x, '\\n\\n', x[0, :], '\\n\\n', x[1:-1, 0], '\\n\\n', x[(0 ,1, 2, 3), (0, 1, 2, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**广播**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]]) \n",
      "\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]]) \n",
      "\n",
      " tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(x, '\\n\\n', y, '\\n\\n', x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设置require_grad属性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True) \n",
      "\n",
      " None \n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x,  '\\n\\n', y.grad, '\\n\\n', x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>) \n",
      "\n",
      " None \n",
      "\n",
      " <AddBackward0 object at 0x7fc118a4c850>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y, '\\n\\n', y.grad, '\\n\\n', y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf, y.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**backward计算梯度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) \n",
      "\n",
      " tensor(27., grad_fn=<MeanBackward0>) \n",
      "\n",
      "\n",
      "None \n",
      "\n",
      " None \n",
      "\n",
      " None \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "z = y * y * 3\n",
    "z_mean = z.mean()\n",
    "print(z, '\\n\\n', z_mean, '\\n\\n')\n",
    "z_mean.backward()  # 等价于 z_mean.backward(torch.tensor(1.))\n",
    "print(z_mean.grad, '\\n\\n', y.grad, '\\n\\n', x.grad, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何避免梯度累加**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x_sum = x.sum()\n",
    "x_sum.backward()  # 等价于 x_sum.backward(torch.tensor(1.))\n",
    "print(x.grad)  # 多次反向传播，grad会累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x_sum = x.sum()\n",
    "# x.grad.data.zero_()  # 一般在反向传播之前需把梯度清零\n",
    "x_sum.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何中断梯度追踪**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \n",
      "\n",
      " True \n",
      "\n",
      " False \n",
      "\n",
      " True\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x.data *= 10  # 方法二：假如只想修改tensor的值，但不希望被autograd记录，可以对tensor.data进行操作\n",
    "y1 = 2 * x\n",
    "with torch.no_grad():  # 方法一： 使用torch.no_grad()\n",
    "    y2 = x**3\n",
    "z = y1 + y2\n",
    "print(x.requires_grad, '\\n\\n', y1.requires_grad, '\\n\\n', y2.requires_grad, '\\n\\n', z.requires_grad)\n",
    "z.backward(torch.ones(2, 2), retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5000, 2.5000],\n",
      "        [2.5000, 2.5000]])\n"
     ]
    }
   ],
   "source": [
    "z_mean  = z.mean()\n",
    "z_mean.backward()\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
